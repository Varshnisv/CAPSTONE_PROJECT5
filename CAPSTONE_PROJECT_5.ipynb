{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOegzurtcpSYOm6Z0GSXVgH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Varshnisv/CAPSTONE_PROJECT5/blob/main/CAPSTONE_PROJECT_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CAPSTONE PROJECT 5\n",
        "DONE BY: VARSHNI S V"
      ],
      "metadata": {
        "id": "zCNSi7PpHf9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End Course Summative Assignment\n",
        "\n",
        "**Problem Statement: Write the Solutions to the Top 50 Interview Questions and Explain any 5 Questions in a Video**\n",
        "\n",
        "Imagine you are a dedicated student aspiring to excel in job interviews. Your task is to write the solutions for any 50 interview questions out of  80 total questions  presented to you. Additionally, create an engaging video where you thoroughly explain the answers to any five of these questions.\n",
        "\n",
        "Your solutions should be concise, well-structured, and effective in showcasing your problem-solving skills. In the video, use a dynamic approach to clarify the chosen questions, ensuring your explanations are easily comprehensible for a broad audience.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "1. Make a copy of this document and write your answers.\n",
        "2. Include the Video Link here in your document before submitting."
      ],
      "metadata": {
        "id": "zQQyluc5G_GJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GITHUB LINK:"
      ],
      "metadata": {
        "id": "dZGqMYbTHGu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. What is a vector in mathematics?**\n",
        "\n",
        "Different from a mere integer, a vector is a mathematical object that possesses both magnitude and direction. Arrows are frequently used to depict vectors; the direction of the arrow denotes the direction of the vector, and the length of the arrow represents the magnitude of the vector. Coordinates are used to represent vectors, which can exist in two-dimensional (2D) or three-dimensional (3D) environments. A vector v in two dimensions, for instance, may be expressed as v = (3, 4), where 3 stands for the x-component and 4 for the y-component. Vectors are employed in physics to express quantities where magnitude and direction are important, such as force and velocity.\n",
        "\n",
        "### **2. How is a vector different from a scalar?**\n",
        "\n",
        "A vector has both magnitude and direction, whereas a scalar is a quantity that just has magnitude. Simple numbers like 5 that describe quantities without direction, like mass, time, or temperature, are called scalars. In contrast, vectors provide direction, which adds extra information. For instance, a location's temperature may be 20¬∞C (a scalar), but a wind blowing 10 km/h in a northeastern direction is a vector since it combines direction and speed. In disciplines like physics, where scalars represent energy or work and vectors properly reflect forces, this distinction is essential.\n",
        "\n",
        "### **3. What are the different operations that can be performed on vectors?**\n",
        "\n",
        "Vectors may undergo a number of operations, each with a distinct function. Through the addition or subtraction of their respective components, vectors can be combined. The vector's length is altered but not its direction when it is scaled by a constant through scalar multiplication. Finding the angle between two vectors is frequently accomplished by multiplying them by a scalar, or dot product. In 3D, the result of the cross product is a new vector that is perpendicular to the first two. The vectors a = (2, 3) and b = (4, -1), for instance, have a + b = (6, 2) after addition, and their dot product is (24 + 3(-1)) = 5.\n",
        "\n",
        "\n",
        "### **4. How can vectors be multiplied by a scalar?**\n",
        "\n",
        "Each vector component is multiplied by the scalar value in order to multiply a vector by a scalar. Unless the scalar is negative, which reverses the direction, this operation modifies the vector's magnitude without changing its direction. For example, the product k * v = (2 * 3, 2 * 4) = (6, 8) is obtained if we have a vector v = (3, 4) and a scalar k = 2. The vector's direction doesn't change even when its length doubles. The vector that results (-3, -4) goes in the opposite direction of v if k = -1.\n",
        "\n",
        "\n",
        "### **5. What is the magnitude of a vector?**\n",
        "\n",
        "A vector's magnitude, often referred to as its length or norm, is the distance in space between the vector's origin and the point it defines. Using the Pythagorean theorem, it is computed. The magnitude |v| for a 2D vector v = (x, y) is:\n",
        "\n",
        "‚à£ùë£‚à£ = ùë• 2 + ùë¶ 2 ‚à£v‚à£ = x 2 + y 2\n",
        "\n",
        "‚Äã\n",
        "\n",
        "For instance, |v| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = 5 if v = (3, 4). When a quantity's strength or intensity is required, such as in physics to calculate force, the magnitude is helpful.\n",
        "\n",
        "\n",
        "### **6. How can the direction of a vector be determined?**\n",
        "\n",
        "A vector's direction is usually shown by the angle it makes with a reference axis, most often the x-axis. Trigonometry may be used to find this angle. The angle Œ∏ with respect to the x-axis for a vector v = (x, y) may be computed as follows:\n",
        "\n",
        "Arctan ‚Å° (ùë¶ùë•) = ùúÉ\n",
        "Œ∏=arctan(x y)\n",
        "For instance, Œ∏ = \\arctan(4/3) ‚âà 53.13¬∞ for v = (3, 4). Practically speaking, the direction indicates the vector's point. Furthermore, direction alone without magnitude can be represented by a unit vector, or a vector of magnitude\n",
        "\n",
        "\n",
        "### **7. What is the difference between a square matrix and a rectangular matrix?**\n",
        "\n",
        "Similar to 2x2 or 3x3 matrices, a square matrix has an equal number of rows and columns. In linear algebra, square matrices are significant due to their potential for unique characteristics like determinants and eigenvalues. Like a 3x2 or 4x3 matrix, a rectangle matrix has a variable number of rows and columns and does not have all of the characteristics of a square matrix. In contrast to B = [[1, 2, 3], [4, 5, 6]], which is a 2x3 rectangle matrix, A = [[1, 2], [3, 4]] is a 2x2 square matrix.\n",
        "\n",
        "\n",
        "### **8. What is a basis in linear algebra?**\n",
        "\n",
        "A set of linearly independent vectors that span a vector space is called a basis in linear algebra. This means that each vector in the space may be represented as a combination of the basis vectors. For instance, the typical basis vectors in 2D space are (0, 1) and (1, 0). These two may be combined to express any 2D vector. For example, 3(1, 0) + 4(0, 1)** can be used to express v = (3, 4). A foundation for uniquely describing any vector in the space is provided by the basis.\n",
        "\n",
        "\n",
        "### **9. What is a linear transformation in linear algebra?**\n",
        "\n",
        "A function that maintains scalar multiplication and vector addition between two vector spaces is called a linear translation. Matrix representations of linear transformations are possible. For example, vector v = (1, 2) scales the x-component by 2 and the y-component by 3 when the transformation matrix T = [[2, 0], [0, 3]] is applied, yielding T(v) = (2, 6). Because they enable systematic object manipulation, linear transformations are essential in physics, engineering, and graphics.\n",
        "\n",
        "\n",
        "### **10. What is an eigenvector in linear algebra?**\n",
        "\n",
        "A non-zero vector that only changes in magnitude‚Äînot direction‚Äîwhen the matrix is applied to it is called an eigenvector. Eigenvectors are defined by the equation A * v = Œª * v, in where A is a matrix, v is an eigenvector, and Œª is the associated eigenvalue. If, for instance, v = (1, 0) and A = [[2, 0], [0, 3]], then A * v = 2 * v, resulting in (1, 0) being an eigenvector with eigenvalue 2. In quantum physics, stability analysis, and data science applications such as Principal Component Analysis (PCA), eigenvectors are crucial.\n",
        "\n",
        "\n",
        "### **11. What is the gradient in machine learning?**\n",
        "\n",
        "A gradient is a vector that shows the function's quickest growth rate and direction. Especially in techniques such as gradient descent, the gradient is crucial for optimisation in machine learning. An rising error is shown by a loss function's gradient with regard to model parameters (weights). By going against the gradient, we may lower the error and raise the accuracy of the model. For instance, the gradient instructs us on how to modify weights in order to minimise prediction error when training a neural network. For a parameter w, the gradient df/dw = 2w if the loss function is f(w) = w^2. To reduce f(w), move w in the opposite direction as the gradient.\n",
        "\n",
        "\n",
        "### **12. What is backpropagation in machine learning?**\n",
        "\n",
        "Backpropagation is a neural network training technique that uses the chain rule to determine the gradient of the loss function with respect to each weight. In order to reduce error, it enables the model to modify weights layer by layer. Backpropagation determines the contribution of each weight to the final error during training, allowing the model to modify these weights appropriately. For instance, backpropagation aids in updating weights in hidden layers of a neural network used for image classification, allowing the model to get better with each iteration. It is essential to deep learning and an effective method of optimising model parameters.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **13. What is the concept of a derivative in calculus?**\n",
        "\n",
        "The pace at which a function changes with respect to one of its variables is represented by a derivative. It basically gauges how a function's output varies with changes in the input. The notation f'(x) or df/dx represents the derivative of a function f(x) at point x. For instance, f'(x) = 2x if f(x) = x^2. The derivative at x = 3 is 6, which indicates that for every unit rise in x close to x = 3, the function grows by 6 units. Fundamental to optimisation, derivatives are employed in machine learning methods to efficiently modify parameters.\n",
        "\n",
        "\n",
        "### **14. How are partial derivatives used in machine learning?**\n",
        "\n",
        "When a function has more than one variable, partial derivatives are used to show how the function changes as one variable changes while the others remain constant. Partial derivatives are used in machine learning to compute the gradients of loss functions with respect to each parameter. This is crucial for gradient descent, where we separately modify each parameter to reduce loss. For instance, the partial derivative with regard to w is ‚àÇL/‚àÇw = 2w, and with respect to b is ‚àÇL/‚àÇb = 2b, if the loss function L(w, b) = w^2 + b^2 for parameters w and b. The amount that w and b should be adjusted in gradient descent is determined by these partial derivatives.\n",
        "\n",
        "\n",
        "### **15. What is probability theory?**\n",
        "\n",
        "A subfield of mathematics known as probability theory examines randomness and uncertainty and offers instruments for simulating and analysing occurrences with unpredictable results. It is fundamental to machine learning, data science, and statistics, where we determine the probability of outcomes or events. According to probability theory, the chance of heads in a fair coin flip, for instance, is 0.5. Probability theory serves as the foundation for machine learning methods such as Bayesian inference and aids in measuring model prediction uncertainty. It serves as the foundation for probability distributions, which explain the distribution of probabilities among various events.\n",
        "\n",
        "\n",
        "### **16. What are the primary components of probability theory?**\n",
        "\n",
        "Probability distributions, occurrences, and outcomes are the main elements of probability theory. Similar to rolling a 4 on a die, an event is a particular result or series of results from a procedure. Potential outcomes include every face of the dice. Different occurrences' probabilities are defined by probability distributions. A six-sided dice roll, for instance, has a 1/6 chance of producing any of the six possible outcomes. Conditional probability, which determines probabilities based on past occurrences, and Bayes' theorem, which modifies beliefs in light of fresh data, are other components of probability theory.\n",
        "\n",
        "\n",
        "### **17. What is conditional probability, and how is it calculated?**\n",
        "\n",
        "The likelihood that an event will occur provided that another event has already occurred is known as conditional probability. P(A|B) is the symbol for the probability of occurrence A given event B. The following formula is used to compute conditional probability:\n",
        "\n",
        "P(A‚à£B)= P(A¬†and¬†B)/P(B)\n",
        "\n",
        "The chance that a student is interested in mathematics given that they are interested in physics is P(Math | Physics) = 0.10 / 0.30 = 1/3, for instance, if we know that 30% of students are interested in mathematics and 10% are interested in both. In domains such as decision-making and data science, conditional probability is essential.\n",
        "\n",
        "\n",
        "### **18. What is Bayes theorem, and how is it used?**\n",
        "\n",
        "How to update a hypothesis' probability in light of fresh data is explained by Bayes' theorem. The text is written as:\n",
        "\n",
        "P(A‚à£B)= P(B‚à£A)‚ãÖP(A)/P(B)\n",
        "\n",
        "where P(A) represents the prior probability, P(B|A) represents the likelihood, and P(B) represents the evidence's probability. In the context of medical diagnostics, for example, if P(Disease|Symptom) is the probability of a disease given a symptom, Bayes' theorem can update this probability according to the frequency of the symptom and its likelihood in healthy people. The Naive Bayes classifier, in particular, makes extensive use of the Bayes theorem in machine learning for classification applications.\n",
        "\n",
        "\n",
        "### **19. What is a random variable, and how is it different from a regular variable?**\n",
        "\n",
        "A regular variable has a defined, predictable value, while a random variable's values are the results of a random phenomena. There are two types of random variables: discrete (having a limited range of potential values) and continuous (having an unlimited range). For instance, X is a discrete random variable that can have values between 1 and 6 if it reflects the result of rolling a die. A distribution defines each value's likelihood. In probability and statistics, random variables are crucial because they allow us to measure event uncertainty.\n",
        "\n",
        "\n",
        "### **20. What is the law of large numbers, and how does it relate to probability theory?**\n",
        "\n",
        "According to the law of large numbers, the average of the outcomes of a random event gets closer to the anticipated value as the number of trials grows. For instance, the percentage of heads and tails will converge to 0.5 if a fair coin is flipped several times. Because bigger samples are more likely to accurately reflect a population's real features, statistical sampling is based on this premise. The rule of big numbers in machine learning guarantees that more robust models are produced from larger datasets.\n",
        "\n",
        "\n",
        "### **21. What is the central limit theorem, and how is it used?**\n",
        "\n",
        "Regardless of the distribution of the population, the central limit theorem (CLT) asserts that as sample size increases, the distribution of sample means approaches a normal distribution. Even in cases when the data does not follow a normal distribution, this characteristic enables us to draw conclusions about population parameters. For example, the mean of sample averages will often follow a normal distribution if we survey the average height of students on a regular basis. The CLT is crucial for statistical confidence interval calculation and hypothesis testing.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **22. What is the difference between discrete and continuous probability distributions?**\n",
        "\n",
        "Random variables that have countable outcomes, as the outcome of a dice roll (1, 2, 3, 4, 5, 6), are subject to a discrete probability distribution. A probability density function (PDF) is used to depict a continuous probability distribution, which is applicable to variables such as temperature or height that can have any value within an interval. For example, there is no chance of striking 5.5 feet squarely, but we may calculate the likelihood of falling between 5.4 and 5.6 feet. Whereas the normal distribution is a continuous distribution, the binomial distribution is a discrete distribution.\n",
        "\n",
        "\n",
        "### **23. What are some common measures of central tendency, and how are they calculated?**\n",
        "\n",
        "By locating a centre point within a dataset, measures of central tendency offer an overview of the data. By adding up all of the values and dividing by the total number of values, the mean (average) is determined. When data is sorted, the median is the middle number that splits the dataset in half. The value that appears the most frequently is the mode. For instance, the mean is 6.75, the median is 7, and the mode is 7 in the dataset [3, 7, 7, 10]. These metrics aid in comprehending a dataset's usual value and dispersion.\n",
        "\n",
        "\n",
        "### **24. What is the purpose of using percentiles and quartiles in data summarization?**\n",
        "\n",
        "\n",
        "By dividing data into segments, percentiles and quartiles provide information about the distribution. Whereas quartiles split data into four equal portions, percentiles split it into 100. For instance, the 75th percentile (3rd quartile) includes 75% of the data, whereas the 25th percentile (1st quartile) indicates the number below which 25% of the data falls. A score in the 90th percentile of a test score collection indicates that the score is greater than 90% of the others. Outliers, central tendency, and distribution shape may all be determined with the help of percentiles and quartiles.\n",
        "\n",
        "### **25. How do you detect and treat outliers in a dataset?**\n",
        "\n",
        "Data points that deviate far from the norm are known as outliers. The IQR approach, which classifies values below Q1 - 1.5IQR or above Q3 + 1.5IQR as outliers, and the Z-score, which classifies values more than three standard deviations as outliers, are two detection techniques. Outlier removal, data modification (such as log transformation), and median or mean imputing are examples of treatment techniques. For instance, using the IQR approach might help make data more representative in a pay dataset where extreme values may be outliers that distort results.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **26. How do you use the central limit theorem to approximate a discrete probability distribution?**\n",
        "\n",
        "According to the Central Limit Theorem (CLT), regardless of the population distribution, as sample size grows, the sampling distribution of the sample mean approaches a normal distribution. For instance, as sample size increases, the mean height distribution gets closer to normal if we repeatedly sample the heights of 50 individuals from a skewed distribution. This is essential for confidence intervals and hypothesis testing because it permits the use of normal distribution features for approximations, enabling statistical inferences on non-normal populations.\n",
        "\n",
        "\n",
        "### **27. How do you test the goodness of fit of a discrete probability distribution?**\n",
        "\n",
        "To determine if observed data fits a certain distribution, the goodness of fit test is used. By comparing observed and predicted frequencies under the proposed distribution, the chi-square test is widely used. By comparing observed and predicted roll frequencies, for instance, we may determine if dice rolls have a uniform distribution (1/6 for each face). We may reject the fit hypothesis if the discrepancies are substantial. Model validation uses goodness of fit tests to make sure data patterns match assumptions, which is essential for precise forecasts.\n",
        "\n",
        "\n",
        "### **28. What is a joint probability distribution?**\n",
        "\n",
        "The likelihood that two or more random variables will occur at the same time is represented by a joint probability distribution. It establishes odds for every possible combination of variable outcomes. For instance, each combination (such as rain and hot) has a probability in a joint distribution for temperature (hot/cold) and rain (yes/no). For events X and Y, it is written as P(X and Y). Because they aid in evaluating the connections and interactions between variables, such as in bivariate analysis, joint distributions are essential to statistics.\n",
        "\n",
        "\n",
        "### **29. How do you calculate the joint probability distribution?**\n",
        "\n",
        "Calculating the probabilities for every conceivable combination of two or more variables yields the joint probability distribution. P(X and Y) = P(X) * P(Y) is the combined probability for two independent occurrences, X and Y. Make use of conditional probabilities for dependent occurrences. For instance, P(Red and Face) = 1/2 * 3/13 = 3/26 is the combined chance of getting a face card (P(Face) = 3/13) and a red card (P(Red) = 1/2). In statistics, joint distributions aid in the analysis of combined results.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **30. What is the difference between a joint probability distribution and a marginal probability distribution?**\n",
        "\n",
        "\n",
        "An individual variable's probability, independent of other factors, is provided by a marginal probability distribution, but a joint probability distribution offers probabilities for combinations of two or more variables. Add the joint probability of the values of the other variables to determine the marginal probabilities. The marginal probability of rain (P(Rain)) takes into account all temperature scenarios, whereas the combined probability of rain and temperature, for instance, displays rain/hot combinations. Joint distributions are frequently used to calculate marginal probabilities, which summarise variable likelihoods.\n",
        "\n",
        "\n",
        "### **31. What is the covariance of a joint probability distribution?**\n",
        "\n",
        "Covariance measures the degree to which two variables change together. For random variables X and Y, covariance is calculated as Cov(X, Y) = E[(X - ŒºX)(Y - ŒºY)], where ŒºX and ŒºY are means of X and Y. Positive covariance indicates that as one variable increases, the other also tends to increase. For example, height and weight may have positive covariance. Covariance values don‚Äôt indicate strength of association like correlation, but they show the direction of relationship between variables.\n",
        "\n",
        "### **32. How do you determine if two random variables are independent based on their joint probability distribution?**\n",
        "\n",
        "Two random variables are independent if their joint probability distribution equals the product of their marginal probabilities for all possible outcomes. Mathematically, X and Y are independent if P(X and Y) = P(X) * P(Y) for each outcome. For example, flipping two coins is independent if the probability of both heads equals P(Heads1) * P(Heads2) = 0.5 * 0.5 = 0.25. Independence simplifies calculations and modeling in statistics, as knowing one variable doesn‚Äôt give information about the other.\n",
        "\n",
        "### **33. What is the relationship between the correlation coefficient and the covariance of a joint probability distribution?**\n",
        "\n",
        "The correlation coefficient quantifies the strength and direction of the linear relationship between two variables and is derived from covariance. It‚Äôs calculated as œÅ(X, Y) = Cov(X, Y) / (œÉX * œÉY), where œÉX and œÉY are standard deviations of X and Y. Correlation values range from -1 to +1, with 0 indicating no linear relationship. For example, height and weight might have a correlation of 0.8, indicating a strong positive relationship. Correlation standardizes covariance, making it easier to interpret relationships.\n",
        "\n",
        "### **34. What is sampling in statistics, and why is it important?**\n",
        "\n",
        "The process of choosing a subset of people to represent a population is known as sampling. Analysing and drawing conclusions about a population without surveying every member is made possible by sampling. To forecast election results, for instance, 1,000 people are surveyed to represent the general population. Because sample analysis enables inferences about a population, sampling is crucial for accuracy, feasibility, and efficiency. Because it allows for insights with few resources, it is essential in statistics, research, and quality control.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **35. What are the different sampling methods commonly used in statistical inference?**\n",
        "\n",
        "Simple random sampling (equal chance for everyone), stratified sampling (division into subgroups), cluster sampling (random selection of clusters), and systematic sampling (selection of every nth person) are examples of sampling techniques used in statistics. Stratified sampling, for instance, guarantees participation from several departments in a business survey, whereas cluster sampling may choose particular teams. The reliability of the results and the correctness of the inference are affected by the technique choice, which is determined by the data properties, sample representativeness, and research objectives.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **36. What is the central limit theorem, and why is it important in statistical inference?**\n",
        "\n",
        "According to the Central Limit Theorem (CLT), if the samples are independent and have the same distribution, the distribution of sample means will resemble a normal distribution as sample size increases, irrespective of the distribution of the population. For instance, if samples of more than thirty test results are taken frequently, the average of each sample will create a normal distribution. Because it enables us to estimate population characteristics and utilise the attributes of a normal distribution to create confidence intervals and conduct hypothesis testing even in cases where the underlying data is not normally distributed, CLT is crucial for inference.\n",
        "\n",
        "\n",
        "### **37. What is the difference between parameter estimation and hypothesis testing?**\n",
        "\n",
        "Calculating a population parameter from sample data, such as the variance or mean, is known as parameter estimation. This process frequently yields an interval estimate (confidence interval) or a single estimate (point estimation). Contrarily, hypothesis testing uses sample data to assess a certain claim or hypothesis on a population parameter and decide whether the evidence supports or refutes it. For instance, parameter estimation is used to determine the average weight of pupils in a class, whereas hypothesis testing is used to determine if the average weight is more than 70 kg. In statistical inference, both techniques are essential for comprehending population features and verifying assertions.\n",
        "\n",
        "\n",
        "### **38. What is the p-value in hypothesis testing?**\n",
        "\n",
        "Assuming the null hypothesis is correct, the p-value shows the likelihood of finding outcomes that are as severe as or more extreme than those in the sample. The null hypothesis is rejected when the observed data is deemed implausible by a small p-value, often less than 0.05. When determining whether a coin is fair, for instance, a p-value of 0.02 means that there is only a 2% probability of receiving outcomes that are as biassed as those that would be expected if the coin is truly fair. A key component of statistical significance testing, the p-value directs judgements based on the strength of the evidence.\n",
        "\n",
        "\n",
        "### **39. What is confidence interval estimation?**\n",
        "\n",
        "With a particular confidence level (e.g., 95%), a confidence interval (CI) calculates the range that a population parameter most likely falls inside. With a 95% CI, about 95 out of 100 samples would include the correct value. For example, we have a 95% confidence level that the population mean height falls between 158 and 162 cm if the sample mean height is 160 cm with a 95% CI of [158, 162]. Unlike a single point estimate, confidence intervals give a range of reasonable values for a parameter, providing additional information.\n",
        "\n",
        "\n",
        "### **40. What are Type I and Type II errors in hypothesis testing?**\n",
        "\n",
        "When the null hypothesis is incorrectly rejected in hypothesis testing, we discover an effect that doesn't exist, which is known as a Type I mistake (false positive). When we overlook an existent effect and fail to reject a false null hypothesis, we commit a Type II mistake, often known as a false negative. A Type I error, for instance, might incorrectly infer that a drug is effective when it isn't, but a Type II error would miss the treatment's true usefulness. Accurate results depend on minimising these mistakes, which is frequently accomplished by boosting sample size and modifying significant thresholds.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **41. What is the difference between correlation and causation?**\n",
        "\n",
        "While correlation shows a statistical relationship between two variables, causation suggests that changes in one variable cause changes in the other. For instance, drowning rates and ice cream sales may be related since they both increase during the summer, but none causes the other. In order to prove a cause-and-effect link and eliminate confounding variables, experimental or longitudinal investigations are necessary to establish causality. Inaccurate conclusions regarding data associations might result from confusing correlation with causation, therefore statistical approaches should appropriately discriminate between the two.\n",
        "\n",
        "\n",
        "### **42. How is a confidence interval defined in statistics?**\n",
        "\n",
        "A range obtained from sample data that most likely contains the real population parameter and linked to a confidence level (such as 90% or 95%) is called a confidence interval (CI). For example, a sample's mean test scores may have a 95% confidence interval (CI) of [80, 90], indicating that the real mean is 95% of the time within this range. The sample size and variability determine the interval's width; bigger samples often provide smaller intervals. By giving a range rather than a single point number, confidence intervals provide a means of quantifying uncertainty in estimations.\n",
        "\n",
        "\n",
        "### **43. What does the confidence level represent in a confidence interval?**\n",
        "\n",
        "The percentage of intervals that would include the population parameter if sampling were done more than once is indicated by the confidence level in a confidence interval. A 95% confidence level, for instance, indicates that if we sampled again, the correct value would probably be present in 95 out of 100 periods. It indicates our confidence in the process, not a 95% likelihood for a given period. Wider intervals result from higher confidence levels, which compromise accuracy for assurance in including the genuine value.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **44. What is hypothesis testing in statistics?**\n",
        "\n",
        "A statistical technique for assessing presumptions (hypotheses) on population parameters based on sample data is hypothesis testing. A null hypothesis (H0) and an alternative hypothesis (H1) are established, and evidence is used to determine whether or not to reject H0. When determining if a new medication is more effective than a placebo, for instance, H0 would be set to \"no difference\" and H1 to \"drug is more effective.\" We either reject or fail to reject H0 based on a computed p-value and a predefined significance level (e.g., 0.05). Making judgements based on evidence is aided by hypothesis testing.\n",
        "\n",
        "\n",
        "### **45. What is the purpose of a null hypothesis in hypothesis testing?**\n",
        "\n",
        "In hypothesis testing, the null hypothesis (H0) is a baseline or default assertion that claims there is no impact, no change, or the status quo. Until there is evidence to the contrary, it is presumed to be true. For instance, H0 would indicate that there is a 50% likelihood of heads when evaluating the fairness of a coin. Statistical tests quantify the degree of evidence required to reject a hypothesis, and H0 serves as a baseline against which findings may be evaluated. The alternative hypothesis is supported when H0 is rejected, which aids in the objective and rigorous evaluation of claims.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **46. What is the difference between a one-tailed and a two-tailed test?**\n",
        "\n",
        "By determining whether a parameter is larger or less than a certain value, but not both, a one-tailed test evaluates the direction of an effect. For instance, it would be one-tailed to investigate if a new medicine improves recovery rates. Without indicating a direction, a two-tailed test assesses both directions to determine if a parameter is higher or less than a value. For example, determining whether a medicine has a favourable or negative impact on recovery rates is a two-tailed test. The decision is based on the hypothesis; one-tailed tests need a good reason but are more effective at identifying directional effects.\n",
        "\n",
        "\n",
        "### **47. What is experiment design, and why is it important?**\n",
        "\n",
        "Planning how to carry out an experiment in order to guarantee accurate, trustworthy, and objective findings is known as experimental design. It include formulating the hypothesis, choosing participants, managing variables, and deciding on the methods for gathering and evaluating data. For instance, experimental design makes ensuring that blinding, control groups, and randomisation are utilised to reduce bias while evaluating a new medication. To maximise the study's capacity to identify real effects, avoid confounding, and draw meaningful findings, good design is crucial.\n",
        "\n",
        "\n",
        "### **48. What are the key elements to consider when designing an experiment?**\n",
        "\n",
        "Replication (repeating the experiment to confirm findings), blinding (hiding information to prevent bias), control (using control groups for comparison), randomisation (adjusting groups at random to reduce bias), and sample size (enough for statistical power) are important components of experiment design. To lessen bias, for example, a placebo group and blinding experimenters and participants are used in medication effectiveness studies. These components are essential for guaranteeing the reliability, validity, and generalisability of experimental results.\n",
        "\n",
        "\n",
        "### **49. How can sample size determination affect experiment design?**\n",
        "\n",
        "The power of the experiment, or its capacity to identify real effects, is impacted by the choice of sample size. While a large sample may identify even insignificant effects, a small sample may lack statistical power and produce false negatives. The allowable margin of error, confidence level, and estimated impact size all affect sample size. For instance, a general satisfaction survey could require fewer respondents, while a research assessing a rare medication side effect requires a large sample to catch enough instances. Reliability of results and cost effectiveness are improved by precise sample size estimate.\n",
        "\n",
        "\n",
        "### **50. What are some strategies to mitigate potential sources of bias in experiment design?**\n",
        "\n",
        "Use blinding (keeping information from participants and experimenters), control groups (comparing to untreated groups), matching (matching individuals with comparable features), and randomisation (random assignment to groups) to reduce bias in experimental design. Double-blind randomisation with a placebo control group, for instance, lessens selection and confirmation biases in clinical trials. Moreover, employing many observers and standardising data gathering methods might help reduce observer bias. These tactics guarantee that actual impacts, not outside influences, are reflected in the results.\n",
        "\n",
        "\n",
        "### **51. What is the geometric interpretation of the dot product?**\n",
        "\n",
        "In geometry, the dot product of two vectors yields a scalar by multiplying their magnitudes by the cosine of their angle. For instance, the dot product of vectors A and B, if they form an angle Œ∏, is |A||B|cos(Œ∏). The dot product is maximised at 0¬∞, which indicates alignment, and zero at 90¬∞, which indicates orthogonality. The force exerted in the direction of displacement is an example of work that is calculated in physics using the dot product. It displays alignment and the direction of one vector's effect on another.\n",
        "\n",
        "\n",
        "### **52. What is the geometric interpretation of the cross-product?**\n",
        "\n",
        "When two vectors are cross-producted, a third vector perpendicular to both is produced, whose magnitude is equal to the area of the parallelogram the vectors create. The cross product magnitude of vectors A and B is |A||B|sin(Œ∏), where Œ∏ is the angle between them. In physics, for instance, rotational effects are demonstrated by the cross product, which establishes the direction and magnitude of torque. In terms of geometry, it is employed to determine a perpendicular vector and compute areas in three dimensions, demonstrating the direction and interplay of vectors.\n",
        "\n",
        "\n",
        "### **53. How are optimization algorithms with calculus used in training deep learning models?**\n",
        "\n",
        "Calculus is used by optimisation techniques such as gradient descent to modify model parameters in order to minimise the error function. Calculus is used in deep learning to estimate the amount and direction of weight changes during training in order to minimise error and achieve convergence towards ideal model parameters. For example, gradient descent iteratively updates weights by using partial derivatives to determine the steepest descent direction. In order to improve model accuracy in tasks like image or voice recognition, techniques like as backpropagation use derivatives of activation functions to modify weights across layers.\n",
        "\n",
        "\n",
        "### **54. What are observational and experimental data in statistics?**\n",
        "\n",
        "Without changing any factors, observational data is gathered by looking at actual situations, such as customer satisfaction survey results. However, controlled studies that change variables to show cause-and-effect relationships‚Äîsuch as assessing the efficacy of drugs with control and treatment groups‚Äîare the source of experimental data. Experiments provide better control and more trustworthy proof of causality, whereas observational studies are frequently less costly but susceptible to confounding factors. Study objectives, practicality, and ethical issues all play a role in which option is selected.\n",
        "\n",
        "\n",
        "### **55. How are confidence tests and hypothesis tests similar? How are they different?**\n",
        "\n",
        "Although they do so in various ways, hypothesis tests and confidence intervals both evaluate population parameters. Confidence intervals provide information about the uncertainty of an estimate by estimating a range that most likely contains the parameter. By comparing sample data to a null hypothesis, hypothesis tests assess assertions and determine whether to accept or reject them. For instance, a hypothesis test may determine if the population mean is equal to 50, but a confidence interval might estimate the mean to be between 50 and 55. Hypothesis tests produce binary conclusions based on statistical significance, whereas CIs offer range-based estimations.\n",
        "\n",
        "\n",
        "### **56. What is the left-skewed distribution and the right-skewed distribution?**\n",
        "\n",
        "The majority of values are concentrated on the right side of a distribution that is left-skewed, with a longer tail on the left. A right-skewed distribution, on the other hand, has the majority of values on the left and a larger tail on the right. For instance, the majority of individuals make moderate earnings, while a smaller percentage earn extraordinarily high sums, resulting in a generally right-skewed income distribution. Skewness impacts metrics like the mean and median and denotes data imbalance. To guarantee accurate interpretation of central tendency and spread, skewed data necessitates extra care during analysis.\n",
        "\n",
        "\n",
        "### **57. What is Bessel‚Äôs correction?**\n",
        "\n",
        "By substituting n - 1 for n in the denominator, Bessel's adjustment modifies the sample variance computation to more accurately estimate population variance. It rectifies sample variance's propensity to understate population variation, particularly in small samples. Bessel's adjustment, for instance, lessens bias when estimating the population variance from a sample of test results. Accurate inference and statistical analysis depend on the sample variance being an unbiased estimator of the population variance, which is achieved by this adjustment.\n",
        "\n",
        "\n",
        "### **58. What is kurtosis?**\n",
        "\n",
        "Kurtosis quantifies a distribution's \"tailedness,\" which suggests the existence of outliers. A normal distribution is said to be mesokurtic if it contains zero excess kurtosis. Distributions with low kurtosis (platykurtic) have light tails and fewer extremes, whereas those with high kurtosis (leptokurtic) have heavy tails and more extreme values. For instance, stock returns are more likely to exhibit extreme values and frequently exhibit leptokurtic behaviour. Kurtosis aids in evaluating the distribution of data and spotting anomalous trends or possible hazards in domains such as quality assurance and finance.\n",
        "\n",
        "\n",
        "### **59. What is the probability of throwing two fair dice when the sum is 5 and 8?**\n",
        "\n",
        "Examine the following pairings to determine the likelihood of obtaining a sum of 5: (1,4), (2,3), (3,2), and (4,1). For a total of 5, there are four possible possibilities. The pairs that yield five results for a sum of eight are (2,6), (3,5), (4,4), (5,3), and (6,2). When two dice are rolled, there are 36 different outcomes, hence the likelihood of receiving a total of 5 is 4/36 and for 8 is 5/36.In statistical models, games of chance, and decision-making, probabilities aid in assessing and forecasting results.\n",
        "\n",
        "\n",
        "\n",
        "### **60. What is the difference between Descriptive and Inferential Statistics?**\n",
        "\n",
        "By using metrics like mean, median, and standard deviation to summarise data, descriptive statistics highlight the key features of a dataset without extrapolating conclusions. However, inferential statistics employ regression, confidence intervals, and hypothesis testing to draw conclusions or forecasts about a broader population based on sample data. It is descriptive to determine the average age of a survey group, for instance, but inferential to use that information to determine the average age of a population. Inferential statistics expand findings to wider contexts, yet both forms offer insights.\n"
      ],
      "metadata": {
        "id": "jD8JOqGHGysK"
      }
    }
  ]
}